<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI八大神经网络简述</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, "Helvetica Neue", Arial, sans-serif;
            line-height: 1.8;
            color: #333;
            max-width: 900px;
            margin: 0 auto;
            padding: 40px 20px;
            background: #f9f9f9;
        }
        .container {
            background: #fff;
            padding: 40px 50px;
            border-radius: 8px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.05);
        }
        h1 {
            font-size: 2.2em;
            margin-bottom: 15px;
            color: #1a1a1a;
            border-bottom: 3px solid #3b82f6;
            padding-bottom: 10px;
        }
        .meta {
            color: #666;
            margin-bottom: 30px;
            font-size: 0.95em;
        }
        .intro {
            background: #f0f9ff;
            padding: 20px;
            border-radius: 6px;
            border-left: 4px solid #3b82f6;
            margin-bottom: 40px;
        }
        .intro h2 {
            font-size: 1.3em;
            margin-bottom: 10px;
        }
        .network {
            margin-bottom: 50px;
            padding: 30px;
            background: #fafafa;
            border-radius: 8px;
            border: 1px solid #e5e5e5;
        }
        .network h2 {
            font-size: 1.6em;
            color: #1e40af;
            margin-bottom: 15px;
        }
        .network-header {
            display: flex;
            gap: 20px;
            margin-bottom: 15px;
            flex-wrap: wrap;
        }
        .tag {
            display: inline-block;
            padding: 4px 12px;
            background: #e0e7ff;
            color: #3730a3;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 500;
        }
        .section {
            margin-bottom: 20px;
        }
        .section h3 {
            font-size: 1.1em;
            color: #475569;
            margin-bottom: 10px;
        }
        .section ul {
            margin-left: 20px;
        }
        .section li {
            margin-bottom: 8px;
        }
        .pros-cons {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin-top: 15px;
        }
        .pros, .cons {
            padding: 15px;
            border-radius: 6px;
        }
        .pros {
            background: #dcfce7;
        }
        .cons {
            background: #fee2e2;
        }
        .pros h4, .cons h4 {
            font-size: 1em;
            margin-bottom: 10px;
        }
        .pros h4 { color: #166534; }
        .cons h4 { color: #991b1b; }
        @media (max-width: 600px) {
            .pros-cons {
                grid-template-columns: 1fr;
            }
            .container {
                padding: 20px;
            }
            body {
                padding: 20px 10px;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>AI八大神经网络简述</h1>
        <p class="meta">发布于 2024-01-01 | 分类: AI | 标签: 神经网络, 深度学习</p>

        <div class="intro">
            <h2>前言</h2>
            <p>神经网络是人工智能领域的核心技术，从1957年感知机诞生到2018年图神经网络的出现，历经六十余年发展。本文梳理八大经典神经网络架构，帮助理解AI技术的演进脉络。</p>
        </div>

        <!-- 1. 感知机 -->
        <div class="network">
            <h2>1. 感知机 (Perceptron)</h2>
            <div class="network-header">
                <span class="tag">年份: 1957</span>
                <span class="tag">提出者: Frank Rosenblatt</span>
            </div>
            <div class="section">
                <h3>核心原理</h3>
                <p>最基础的神经网络单元，模拟单个神经元的工作方式。接收输入信号，加权求和后通过激活函数输出。</p>
                <p><strong>公式</strong>: y = f(Σ(wᵢxᵢ) + b)</p>
            </div>
            <div class="section">
                <h3>应用场景</h3>
                <ul>
                    <li>简单的二分类问题（线性可分）</li>
                    <li>逻辑门电路实现</li>
                </ul>
            </div>
            <div class="pros-cons">
                <div class="pros">
                    <h4>优点</h4>
                    <ul>
                        <li>结构简单，易于理解</li>
                        <li>计算效率高</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>缺点</h4>
                    <ul>
                        <li>只能解决线性可分问题</li>
                        <li>无法处理XOR等复杂问题</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- 2. 多层感知机 -->
        <div class="network">
            <h2>2. 多层感知机 (MLP)</h2>
            <div class="network-header">
                <span class="tag">年份: 1986</span>
                <span class="tag">突破: 反向传播算法</span>
            </div>
            <div class="section">
                <h3>核心原理</h3>
                <p>在感知机基础上引入隐藏层，通过反向传播算法调整权重，实现非线性映射。</p>
                <p><strong>结构</strong>: 输入层 → 隐藏层(多层) → 输出层</p>
            </div>
            <div class="section">
                <h3>应用场景</h3>
                <ul>
                    <li>通用函数拟合</li>
                    <li>分类与回归任务</li>
                    <li>特征提取与转换</li>
                </ul>
            </div>
            <div class="pros-cons">
                <div class="pros">
                    <h4>优点</h4>
                    <ul>
                        <li>通用近似能力</li>
                        <li>可解决非线性问题</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>缺点</h4>
                    <ul>
                        <li>参数量大，易过拟合</li>
                        <li>难以处理序列和图像数据</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- 3. 卷积神经网络 -->
        <div class="network">
            <h2>3. 卷积神经网络 (CNN)</h2>
            <div class="network-header">
                <span class="tag">年份: 1998 (LeNet-5)</span>
                <span class="tag">代表作: ResNet, VGG</span>
            </div>
            <div class="section">
                <h3>核心原理</h3>
                <p>通过卷积层提取局部特征，池化层降维，全连接层输出结果。具备<strong>局部连接</strong>和<strong>权重共享</strong>特性。</p>
                <p><strong>结构</strong>: 卷积层 → 池化层 → 全连接层</p>
            </div>
            <div class="section">
                <h3>应用场景</h3>
                <ul>
                    <li>图像分类、目标检测</li>
                    <li>人脸识别</li>
                    <li>医疗影像分析</li>
                </ul>
            </div>
            <div class="pros-cons">
                <div class="pros">
                    <h4>优点</h4>
                    <ul>
                        <li>自动提取特征</li>
                        <li>平移不变性</li>
                        <li>参数量相对较少</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>缺点</h4>
                    <ul>
                        <li>需要大量标注数据</li>
                        <li>计算资源消耗大</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- 4. 循环神经网络 -->
        <div class="network">
            <h2>4. 循环神经网络 (RNN)</h2>
            <div class="network-header">
                <span class="tag">年份: 1986</span>
                <span class="tag">特点: 处理序列数据</span>
            </div>
            <div class="section">
                <h3>核心原理</h3>
                <p>神经元之间形成循环连接，隐藏层的输出会作为下一时刻的输入，从而"记住"历史信息。</p>
                <p><strong>公式</strong>: hₜ = f(W·hₜ₋₁ + U·xₜ + b)</p>
            </div>
            <div class="section">
                <h3>应用场景</h3>
                <ul>
                    <li>时间序列预测</li>
                    <li>文本生成</li>
                    <li>语音识别</li>
                </ul>
            </div>
            <div class="pros-cons">
                <div class="pros">
                    <h4>优点</h4>
                    <ul>
                        <li>天然处理变长序列</li>
                        <li>时序信息建模</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>缺点</h4>
                    <ul>
                        <li>梯度消失/爆炸</li>
                        <li>难以学习长期依赖</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- 5. 长短期记忆网络 -->
        <div class="network">
            <h2>5. 长短期记忆网络 (LSTM)</h2>
            <div class="network-header">
                <span class="tag">年份: 1997</span>
                <span class="tag">提出者: Hochreiter & Schmidhuber</span>
            </div>
            <div class="section">
                <h3>核心原理</h3>
                <p>在RNN基础上引入<strong>门控机制</strong>（遗忘门、输入门、输出门），有效解决梯度消失问题，实现长期记忆。</p>
                <p><strong>核心</strong>: 细胞状态 + 三个门控单元</p>
            </div>
            <div class="section">
                <h3>应用场景</h3>
                <ul>
                    <li>机器翻译</li>
                    <li>语音识别与合成</li>
                    <li>文本情感分析</li>
                </ul>
            </div>
            <div class="pros-cons">
                <div class="pros">
                    <h4>优点</h4>
                    <ul>
                        <li>解决长期依赖问题</li>
                        <li>梯度传播稳定</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>缺点</h4>
                    <ul>
                        <li>结构复杂，计算量大</li>
                        <li>训练较慢</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- 6. 生成对抗网络 -->
        <div class="network">
            <h2>6. 生成对抗网络 (GAN)</h2>
            <div class="network-header">
                <span class="tag">年份: 2014</span>
                <span class="tag">提出者: Ian Goodfellow</span>
            </div>
            <div class="section">
                <h3>核心原理</h3>
                <p>两个神经网络相互对抗：<strong>生成器</strong>伪造数据，<strong>判别器</strong>识别真伪，通过博弈共同提升性能。</p>
                <p><strong>目标</strong>: min max V(D, G)</p>
            </div>
            <div class="section">
                <h3>应用场景</h3>
                <ul>
                    <li>图像生成（人脸、风景）</li>
                    <li>风格迁移</li>
                    <li>数据增强</li>
                    <li>深度伪造检测</li>
                </ul>
            </div>
            <div class="pros-cons">
                <div class="pros">
                    <h4>优点</h4>
                    <ul>
                        <li>生成质量极高</li>
                        <li>无需显式建模分布</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>缺点</h4>
                    <ul>
                        <li>训练不稳定（模式崩溃）</li>
                        <li>评估困难</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- 7. Transformer -->
        <div class="network">
            <h2>7. Transformer</h2>
            <div class="network-header">
                <span class="tag">年份: 2017</span>
                <span class="tag">论文: Attention Is All You Need</span>
            </div>
            <div class="section">
                <h3>核心原理</h3>
                <p>完全基于<strong>自注意力机制</strong>（Self-Attention），抛弃循环和卷积，并行处理序列中所有位置的关系。</p>
                <p><strong>核心</strong>: Multi-Head Attention + 位置编码</p>
            </div>
            <div class="section">
                <h3>应用场景</h3>
                <ul>
                    <li>大语言模型（GPT、BERT、Claude）</li>
                    <li>机器翻译</li>
                    <li>代码生成</li>
                </ul>
            </div>
            <div class="pros-cons">
                <div class="pros">
                    <h4>优点</h4>
                    <ul>
                        <li>高度并行化，训练快</li>
                        <li>捕获长程依赖能力强</li>
                        <li>可扩展性强</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>缺点</h4>
                    <ul>
                        <li>对算力要求极高</li>
                        <li>模型参数量巨大</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- 8. 图神经网络 -->
        <div class="network">
            <h2>8. 图神经网络 (GNN)</h2>
            <div class="network-header">
                <span class="tag">年份: 2018</span>
                <span class="tag">代表作: GCN, GAT</span>
            </div>
            <div class="section">
                <h3>核心原理</h3>
                <p>直接在图结构上进行学习，通过<strong>消息传递</strong>机制聚合邻居节点信息，更新节点表示。</p>
                <p><strong>核心</strong>: 节点特征 + 边结构 + 聚合函数</p>
            </div>
            <div class="section">
                <h3>应用场景</h3>
                <ul>
                    <li>社交网络分析</li>
                    <li>分子结构预测（药物研发）</li>
                    <li>推荐系统</li>
                    <li>知识图谱</li>
                </ul>
            </div>
            <div class="pros-cons">
                <div class="pros">
                    <h4>优点</h4>
                    <ul>
                        <li>天然处理图结构数据</li>
                        <li>利用拓扑信息</li>
                    </ul>
                </div>
                <div class="cons">
                    <h4>缺点</h4>
                    <ul>
                        <li>过平滑问题</li>
                        <li>对图扰动敏感</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- 总结 -->
        <div class="network" style="background: #f0f9ff; border-color: #3b82f6;">
            <h2>总结与展望</h2>
            <div class="section">
                <h3>发展脉络</h3>
                <p>从单神经元感知机到深度学习时代，神经网络架构持续演进：感知机 → MLP → CNN/RNN/LSTM → GAN/Transformer → GNN，每一步都在突破前一代架构的局限性。</p>
            </div>
            <div class="section">
                <h3>未来趋势</h3>
                <ul>
                    <li><strong>多模态融合</strong>: 统一处理文本、图像、音频</li>
                    <li><strong>效率优化</strong>: 更小更强的模型架构</li>
                    <li><strong>可解释性</strong>: 理解模型决策过程</li>
                    <li><strong>神经符号结合</strong>: 结合逻辑推理与学习</li>
                </ul>
            </div>
        </div>

    </div>
</body>
</html>
